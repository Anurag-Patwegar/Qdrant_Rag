
# ğŸ“˜ Qdrant Rag

A fully local **Retrieval-Augmented Generation (RAG)** system built using:

- ğŸ§  TinyLlama (LLM inference via llama-cpp-python)
- ğŸ“¦ Qdrant (Vector Database)
- ğŸ” sentence-transformers (Embeddings)
- ğŸ¤– OpenAI-compatible local API server

This project demonstrates how to build an end-to-end ChatGPT-style RAG pipeline completely locally.

---

## ğŸš€ Features

- âœ… Local LLM inference using TinyLlama
- âœ… Semantic search with Qdrant vector database
- âœ… Embedding generation using sentence-transformers
- âœ… End-to-end RAG pipeline
- âœ… Fully local execution (No external API required)

---

## ğŸ§° Prerequisites

| Requirement | Version |
|------------|----------|
| Python | 3.10+ |
| Docker | Installed & Running |
| Git | Installed |
| RAM | 16GB Recommended (for LLM) |

Optional:
- GPU for faster inference
- GitHub Codespaces (if not running locally)

---

# ğŸ“Œ Step-by-Step Setup Guide

## 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Anurag-Patwegar/Qdrant_Rag.git
cd Qdrant_Rag
```

---

## 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

### Activate Environment

**Windows**
```bash
venv\Scripts\activate
```

**Mac/Linux**
```bash
source venv/bin/activate
```

---

## 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## 4ï¸âƒ£ Start Qdrant Vector Database

```bash
docker run -d -p 6333:6333 qdrant/qdrant
```

Qdrant will run on:
```
http://localhost:6333
```

---

## 5ï¸âƒ£ Setup & Start LLM Server

Download a TinyLlama compatible GGUF model file and place it locally.

Start the server:

```bash
python app/server.py
```

âš ï¸ First startup may take a few minutes depending on system performance.

---

## 6ï¸âƒ£ Document Ingestion & Embedding

Before querying:

1. Load documents  
2. Generate embeddings  
3. Store embeddings in Qdrant  

Example embedding logic:

```python
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient

model = SentenceTransformer("all-mpnet-base-v2")
client = QdrantClient(host="localhost", port=6333)

# Generate embeddings and push to Qdrant
```

Run ingestion script (if available):

```bash
python ingest.py
```

---

## 7ï¸âƒ£ Query the RAG System

```bash
python app/query.py "What is Retrieval Augmented Generation?"
```

Pipeline flow:

1. Convert query â†’ embedding  
2. Retrieve relevant vectors from Qdrant  
3. Send retrieved context + query to TinyLlama  
4. Generate contextual answer  

---

# ğŸ§ª End-to-End Execution Order

```bash
# 1. Start Qdrant
docker run -d -p 6333:6333 qdrant/qdrant

# 2. Activate environment
source venv/bin/activate   # or Windows equivalent

# 3. Start LLM server
python app/server.py

# 4. Ingest documents
python ingest.py

# 5. Run query
python app/query.py
```

---

# ğŸ“ Project Structure

```
Qdrant_Rag/
â”‚
â”œâ”€â”€ app/                # LLM server and query logic
â”œâ”€â”€ scripts/            # Utility / ingestion scripts
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ docs/               # Sample documents (if provided)
```

---

# ğŸ›  Troubleshooting

| Issue | Solution |
|-------|----------|
| Qdrant not running | Check Docker container status |
| Port 6333 blocked | Ensure no firewall blocking |
| LLM crashes | Ensure enough RAM |
| Slow inference | Use smaller model or enable GPU |

Check running containers:

```bash
docker ps
```

Stop Qdrant:

```bash
docker stop <container_id>
```

---

# ğŸ’¡ Best Practices

- Use smaller models during development  
- Use GPU acceleration if available  
- Monitor memory usage  
- Keep embeddings consistent (same model for indexing and querying)  

---

# ğŸ“œ License

No explicit license found. Refer to repository for updates.

---

# ğŸ™Œ Contributing

Contributions are welcome!

- Improve ingestion pipeline  
- Add better chunking strategies  
- Add streaming responses  
- Add frontend UI  

Feel free to raise issues or submit pull requests.

---

# ğŸ§  What This Project Demonstrates

- Vector databases  
- Local LLM serving  
- Semantic search  
- End-to-end RAG pipeline  
- Production-style modular architecture  

Perfect for learning LLMOps, RAG architecture, and local AI system deployment.

---

â­ If you find this useful, consider starring the repository!
